{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## scraping"
      ],
      "metadata": {
        "id": "uiMYA7y7ziDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "from urllib.parse import urljoin\n",
        "import logging\n",
        "from typing import Set ,List, Optional ,Dict"
      ],
      "metadata": {
        "id": "HQuAmJBU2ROV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For extracting listing links\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('mubawab_link_extractor.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "class MubawabLinkExtractor:\n",
        "\n",
        "    DEFAULT_HEADERS = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "    }\n",
        "\n",
        "    def __init__(self, base_url: str, output_file: str = \"mubawab_links.csv\"):\n",
        "\n",
        "        self.base_url = base_url\n",
        "        self.headers = self.DEFAULT_HEADERS.copy()\n",
        "        self.session = requests.Session()\n",
        "        self.all_links: Set[str] = set()\n",
        "        self.output_file = output_file\n",
        "\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "    def _request_with_retry(self, url: str, retries: int = 3) -> Optional[str]:\n",
        "\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = self.session.get(url, headers=self.headers, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                return response.text\n",
        "            except Exception as e:\n",
        "                if attempt < retries - 1:\n",
        "                    wait_time = 2 ** attempt\n",
        "                    logging.warning(f\"Attempt {attempt + 1} failed for {url}. Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                logging.error(f\"Failed to fetch {url} after {retries} attempts: {e}\")\n",
        "                return None\n",
        "\n",
        "    def extract_links_from_page(self, page_url: str) -> Set[str]:\n",
        "\n",
        "        html_content = self._request_with_retry(page_url)\n",
        "        if not html_content:\n",
        "            return set()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        new_links = set()\n",
        "\n",
        "        # Try different methods to find listings\n",
        "        listings = soup.find_all('div', class_=['listingBox', 'listingBoxsPremium'])\n",
        "        if not listings:\n",
        "            listings = soup.find_all('a', href=re.compile(r'/fr/[pa]/\\d+'))\n",
        "\n",
        "        if not listings:\n",
        "            script_tags = soup.find_all('script', type='text/javascript')\n",
        "            for script in script_tags:\n",
        "                if 'listingBox' in str(script):\n",
        "                    listings = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', str(script))\n",
        "                    listings = [l for l in listings if re.match(r'/fr/[pa]/\\d+', l)]\n",
        "                    break\n",
        "\n",
        "        for listing in listings:\n",
        "            if hasattr(listing, 'attrs'):\n",
        "                link = listing.find('a', href=True)\n",
        "                href = link['href'] if link else listing.get('linkref', '')\n",
        "            else:\n",
        "                href = listing\n",
        "\n",
        "            if not href:\n",
        "                continue\n",
        "\n",
        "            if not href.startswith('http'):\n",
        "                href = urljoin('https://www.mubawab.ma', href)\n",
        "            clean_url = re.sub(r'\\?.*', '', href)\n",
        "\n",
        "            if re.match(r'https://www.mubawab.ma/fr/[pa]/\\d+', clean_url):\n",
        "                new_links.add(clean_url)\n",
        "\n",
        "        return new_links\n",
        "\n",
        "    def extract_listing_links(self, max_pages: int = 37) -> Set[str]:\n",
        "\n",
        "        for page in range(1, max_pages + 1):\n",
        "            page_url = f\"{self.base_url}:p:{page}\"\n",
        "            logging.info(f\"Scraping page {page}/{max_pages}: {page_url}\")\n",
        "\n",
        "            new_links = self.extract_links_from_page(page_url)\n",
        "            new_count = len(new_links - self.all_links)\n",
        "            self.all_links.update(new_links)\n",
        "\n",
        "            logging.info(f\"Found {len(new_links)} links, {new_count} new unique links\")\n",
        "            logging.info(f\"Total unique links: {len(self.all_links)}\")\n",
        "\n",
        "            self.save_links_to_csv()\n",
        "\n",
        "            time.sleep(random.uniform(3, 4))\n",
        "\n",
        "            if new_count == 0 and page > 5:\n",
        "                logging.info(\"No new links found on multiple pages. Stopping early.\")\n",
        "                break\n",
        "\n",
        "        return self.all_links\n",
        "\n",
        "    def save_links_to_csv(self):\n",
        "\n",
        "        with open(self.output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['URL'])\n",
        "            writer.writerows([[link] for link in sorted(self.all_links)])\n",
        "        logging.info(f\"Saved {len(self.all_links)} unique links to {self.output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    BASE_URL = \"https://www.mubawab.ma/fr/ct/rabat/immobilier-a-vendre\"\n",
        "    OUTPUT_FILE = \"data/mubawab_links.csv\"\n",
        "\n",
        "    extractor = MubawabLinkExtractor(BASE_URL, OUTPUT_FILE)\n",
        "    extractor.extract_listing_links()\n",
        "    logging.info(\"Link extraction completed!\")"
      ],
      "metadata": {
        "id": "gDhUyTlt2fji"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('mubawab_data_extractor.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "class MubawabDataExtractor:\n",
        "    DEFAULT_HEADERS = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "    }\n",
        "\n",
        "    def __init__(self, input_file: str = \"data/mubawab_links.csv\",\n",
        "                 output_file: str = \"data/mubawab_properties.csv\"):\n",
        "        self.headers = self.DEFAULT_HEADERS.copy()\n",
        "        self.session = requests.Session()\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "    def _request_with_retry(self, url: str, retries: int = 3) -> Optional[str]:\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = self.session.get(url, headers=self.headers, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                return response.text\n",
        "            except Exception as e:\n",
        "                if attempt < retries - 1:\n",
        "                    wait_time = 2 ** attempt\n",
        "                    logging.warning(f\"Attempt {attempt + 1} failed for {url}. Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    logging.error(f\"Failed to fetch {url} after {retries} attempts: {e}\")\n",
        "        return None\n",
        "\n",
        "    def load_links(self) -> List[str]:\n",
        "        try:\n",
        "            with open(self.input_file, 'r', encoding='utf-8') as f:\n",
        "                reader = csv.reader(f)\n",
        "                next(reader)  # Skip header\n",
        "                return [row[0] for row in reader if row]\n",
        "        except FileNotFoundError:\n",
        "            logging.error(f\"Input file not found: {self.input_file}\")\n",
        "            return []\n",
        "\n",
        "    def scrape_listing_details(self, url: str) -> Optional[Dict]:\n",
        "        logging.info(f\"Scraping details from: {url}\")\n",
        "        html_content = self._request_with_retry(url)\n",
        "        if not html_content:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        data = {'url': url}\n",
        "\n",
        "        try:\n",
        "            prop_id = re.search(r'/fr/[pa]/(\\d+)', url)\n",
        "            data['id'] = prop_id.group(1) if prop_id else 'N/A'\n",
        "\n",
        "            title = soup.find('h1', class_='titleListing')\n",
        "            data['title'] = title.get_text(strip=True) if title else 'N/A'\n",
        "\n",
        "            price = soup.find('h3', class_='orangeTit')\n",
        "            if price:\n",
        "                price_text = price.get_text(strip=True)\n",
        "                data['price'] = re.sub(r'[^\\d]', '', price_text) or 'N/A'\n",
        "            else:\n",
        "                data['price'] = 'N/A'\n",
        "\n",
        "            location = soup.find('h2', class_='greyTit')\n",
        "            data['location'] = location.get_text(strip=True) if location else 'N/A'\n",
        "\n",
        "            features = {\n",
        "                'area': ('icon-triangle', 'm¬≤'),\n",
        "                'rooms': ('icon-house-boxes', 'Pi√®ces|places'),\n",
        "                'bedrooms': ('icon-bed', 'Chambres'),\n",
        "                'bathrooms': ('icon-bath', 'Salles de bain')\n",
        "            }\n",
        "\n",
        "            for field, (icon_class, pattern) in features.items():\n",
        "                icon = soup.find('i', class_=icon_class)\n",
        "                if icon:\n",
        "                    parent = icon.find_parent('div', class_='adDetailFeature')\n",
        "                    if parent:\n",
        "                        span = parent.find('span')\n",
        "                        if span:\n",
        "                            match = re.search(r'(\\d+)', span.get_text(strip=True))\n",
        "                            data[field] = match.group(1) if match else 'N/A'\n",
        "                        else:\n",
        "                            data[field] = 'N/A'\n",
        "                    else:\n",
        "                        data[field] = 'N/A'\n",
        "                else:\n",
        "                    data[field] = 'N/A'\n",
        "\n",
        "            desc = soup.find('div', class_='blockDescription')\n",
        "            data['description'] = desc.get_text(strip=True) if desc else ''\n",
        "\n",
        "            quartier_element = soup.find('h3', class_='greyTit')\n",
        "            if quartier_element:\n",
        "                quartier_text = quartier_element.get_text(strip=True)\n",
        "                data['quartier'] = quartier_text.split(',')[0].strip()\n",
        "            else:\n",
        "                data['quartier'] = 'N/A'\n",
        "\n",
        "            types = ['appartement', 'maison', 'villa', 'terrain', 'bureau', 'studio']\n",
        "            lower_title = data['title'].lower()\n",
        "            data['type'] = next((t for t in types if t in url.lower() or t in lower_title), 'N/A')\n",
        "\n",
        "            data['property_state'] = 'N/A'\n",
        "            etat_label = soup.find('p', class_='adMainFeatureContentLabel',\n",
        "                                   string=re.compile(r'Etat du bien', re.I))\n",
        "            if etat_label:\n",
        "                etat_value = etat_label.find_next('p', class_='adMainFeatureContentValue')\n",
        "                if etat_value:\n",
        "                    data['property_state'] = etat_value.get_text(strip=True)\n",
        "\n",
        "            amenities = {\n",
        "                'cuisine_equiped': 'Non',\n",
        "                'jardin': 'Non',\n",
        "                'piscine': 'Non',\n",
        "                'terrasse': 'Non',\n",
        "                'garage': 'Non',\n",
        "                'ascenseur': 'Non'\n",
        "            }\n",
        "\n",
        "            feature_divs = soup.find_all('div', class_='adFeature')\n",
        "            for feature_div in feature_divs:\n",
        "                icon = feature_div.find('i')\n",
        "                if not icon:\n",
        "                    continue\n",
        "\n",
        "                icon_classes = icon.get('class', [])\n",
        "                feature_text = feature_div.find('span', class_='fsize11')\n",
        "                feature_text = feature_text.get_text(strip=True) if feature_text else ''\n",
        "\n",
        "                if 'icon-fullKitchen' in icon_classes:\n",
        "                    amenities['cuisine_equiped'] = 'Oui'\n",
        "                elif 'icon-garden' in icon_classes or 'jardin' in feature_text.lower():\n",
        "                    amenities['jardin'] = 'Oui'\n",
        "                elif 'icon-pool' in icon_classes or 'piscine' in feature_text.lower():\n",
        "                    amenities['piscine'] = 'Oui'\n",
        "                elif 'icon-terrace' in icon_classes or 'terrasse' in feature_text.lower():\n",
        "                    amenities['terrasse'] = 'Oui'\n",
        "                elif 'icon-garage' in icon_classes or 'garage' in feature_text.lower():\n",
        "                    amenities['garage'] = 'Oui'\n",
        "                elif 'icon-elevator' in icon_classes or 'ascenseur' in feature_text.lower():\n",
        "                    amenities['ascenseur'] = 'Oui'\n",
        "\n",
        "            description = data.get('description', '').lower()\n",
        "            if amenities['cuisine_equiped'] == 'Non' and ('cuisine √©quip√©e' in description or 'cuisine equipee' in description):\n",
        "                amenities['cuisine_equiped'] = 'Oui'\n",
        "            if amenities['jardin'] == 'Non' and 'jardin' in description:\n",
        "                amenities['jardin'] = 'Oui'\n",
        "            if amenities['piscine'] == 'Non' and 'piscine' in description:\n",
        "                amenities['piscine'] = 'Oui'\n",
        "\n",
        "            data.update(amenities)\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error scraping {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def scrape_all_listings(self):\n",
        "        links = self.load_links()\n",
        "        if not links:\n",
        "            logging.error(\"No links found to process.\")\n",
        "            return\n",
        "\n",
        "        total = len(links)\n",
        "        success_count = 0\n",
        "        fail_count = 0\n",
        "\n",
        "        logging.info(f\"Found {total} listings to scrape.\")\n",
        "\n",
        "        file_exists = os.path.exists(self.output_file)\n",
        "        fieldnames = [\n",
        "            'id', 'url', 'title', 'price', 'location', 'type',\n",
        "            'area', 'rooms', 'bedrooms', 'bathrooms', 'description',\n",
        "            'property_state', 'jardin', 'piscine', 'cuisine_equiped',\n",
        "            'terrasse', 'garage', 'ascenseur', 'quartier', 'status'\n",
        "        ]\n",
        "\n",
        "        with open(self.output_file, 'a' if file_exists else 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            if not file_exists:\n",
        "                writer.writeheader()\n",
        "\n",
        "            for i, link in enumerate(links, 1):\n",
        "                logging.info(f\"‚û°Ô∏è Processing ({i}/{total}): {link}\")\n",
        "                data = self.scrape_listing_details(link)\n",
        "                if data:\n",
        "                    data['status'] = 'success'\n",
        "                    writer.writerow(data)\n",
        "                    f.flush()\n",
        "                    success_count += 1\n",
        "                    logging.info(f\"‚úÖ Successfully scraped {data.get('id', 'N/A')}\")\n",
        "                else:\n",
        "                    writer.writerow({'url': link, 'status': 'failed'})\n",
        "                    fail_count += 1\n",
        "                    logging.warning(f\"‚ùå Failed to scrape: {link}\")\n",
        "\n",
        "                time.sleep(random.uniform(2, 4))\n",
        "\n",
        "        logging.info(f\"üìä Scraping finished: {success_count}/{total} succeeded, {fail_count} failed.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    INPUT_FILE = \"data/mubawab_links.csv\"\n",
        "    OUTPUT_FILE = \"data/mubawab_properties.csv\"\n",
        "\n",
        "    extractor = MubawabDataExtractor(INPUT_FILE, OUTPUT_FILE)\n",
        "    extractor.scrape_all_listings()\n",
        "    logging.info(\"Data extraction completed!\")"
      ],
      "metadata": {
        "id": "w42ZLGww46X_"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}