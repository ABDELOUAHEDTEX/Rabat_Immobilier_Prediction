import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import random
from tqdm import tqdm
import os
# Chemin du r√©pertoire pour stocker les donn√©es
SCRIPT_DIR = "Rabat_Immobilier_Prediction\\Model\\Scraping"

# Dossier pour stocker les donn√©es
DATA_DIR = SCRIPT_DIR

def get_listing_links(page_url):
    """Returns: list: Liste des liens des annonces immobili√®res trouv√©es sur la page"""
    try:
        # Ajout d'un d√©lai pour √©viter de surcharger le serveur
        time.sleep(random.uniform(1, 3))
        # Envoi de la requ√™te HTTP avec les en-t√™tes simulant un navigateur
        response = requests.get(page_url, timeout=30)
        
        if response.status_code == 200:
            # Analyse du contenu HTML avec BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Recherche des annonces immobili√®res dans la liste
            listing_container = soup.find('div', class_='ulListing')
            
            if listing_container:
                listings = listing_container.find_all('div', class_='listingBox')
                return [listing['linkref'] for listing in listings if 'linkref' in listing.attrs]
            else:
                print(f"Aucune liste d'annonces trouv√©e sur la page: {page_url}")
                return []
        else:
            print(f"√âchec de r√©cup√©ration de la page. Code de statut: {response.status_code}")
            return []
    except Exception as e:
        print(f"Erreur lors de l'extraction des liens: {e}")
        return []

def extract_listing_details(link):
    """Args:link: URL de l'annonce immobili√®re
    Returns:dict: Dictionnaire contenant les d√©tails de l'annonce"""
    # Initialisation des donn√©es par d√©faut
    property_data = {
        'Prix': 'N/A',
        'Quartier': 'N/A',
        'Superficie': 'N/A',
        'Pi√®ces': 'N/A',
        'Chambres': 'N/A',
        'Salles_de_bain': 'N/A',
        'Type_bien': 'N/A',
        'Etat': 'N/A',
        'Standing': 'N/A',
        'Etat_bien': 'N/A',
        'Ascenseur': 0,
        'Garage': 0,
        'Jardin': 0,
        'Piscine': 0,
        'Cuisine_√©quip√©e': 0,
        'Vue_sur_mer': 0,
        'Terrasse': 0,
        'S√©curit√©': 0,
        'URL': link
    }
    
    try:
        # Ajout d'un d√©lai pour √©viter de surcharger le serveur
        time.sleep(random.uniform(1, 3))
        
        # Envoi de la requ√™te HTTP
        response = requests.get(link, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraction du prix
            price_tag = soup.find('h3', class_='orangeTit')
            if price_tag:
                property_data['Prix'] = ''.join(filter(str.isdigit, price_tag.get_text(strip=True)))
            
            # Extraction du quartier (localisation)
            quartier_div = soup.find('div', class_='col-8 vAlignM adBread')
            if quartier_div:
                quartier_tags = quartier_div.find_all('a', class_='darkblue')
                if quartier_tags:
                    property_data['Quartier'] = quartier_tags[-1].get_text(strip=True)
            
            # Extraction des d√©tails √† partir des ic√¥nes
            details_div = soup.find('div', class_='disFlex adDetails')
            if details_div:
                details = details_div.find_all('div', class_='adDetailFeature')
                for detail in details:
                    icon = detail.find('i', class_='adDetailFeatureIcon')
                    value = detail.find('span').get_text(strip=True) if detail.find('span') else 'N/A'
                    
                    if icon and 'class' in icon.attrs:
                        # Extraction de la superficie
                        if 'icon-triangle' in icon['class']:
                            superficie_match = re.findall(r'\d+', value)
                            property_data['Superficie'] = superficie_match[0] if superficie_match else 'N/A'
                        
                        # Extraction du nombre de pi√®ces
                        elif 'icon-house-boxes' in icon['class']:
                            pieces_match = re.findall(r'\d+', value)
                            property_data['Pi√®ces'] = pieces_match[0] if pieces_match else 'N/A'
                        
                        # Extraction du nombre de chambres
                        elif 'icon-bed' in icon['class']:
                            chambres_match = re.findall(r'\d+', value)
                            property_data['Chambres'] = chambres_match[0] if chambres_match else 'N/A'
                        
                        # Extraction du nombre de salles de bain
                        elif 'icon-bath' in icon['class']:
                            sdb_match = re.findall(r'\d+', value)
                            property_data['Salles_de_bain'] = sdb_match[0] if sdb_match else 'N/A'
            
            # Extraction des caract√©ristiques principales
            features_div = soup.find('div', class_='adFeatures')
            if features_div:
                main_features = features_div.find_all('div', class_='adMainFeature')
                for feature in main_features:
                    label_tag = feature.find('p', class_='adMainFeatureContentLabel')
                    value_tag = feature.find('p', class_='adMainFeatureContentValue')
                    
                    if label_tag and value_tag:
                        label = label_tag.get_text(strip=True)
                        value = value_tag.get_text(strip=True)
                        
                        # Assignation des valeurs selon les √©tiquettes
                        if label == 'Type de bien':
                            property_data['Type_bien'] = value
                        elif label == 'Etat':
                            property_data['Etat'] = value
                        elif label == 'Standing':
                            property_data['Standing'] = value
                        elif label == 'Etat du bien':
                            property_data['Etat_bien'] = value
                
                # Extraction des caract√©ristiques suppl√©mentaires
                extra_tags_div = features_div.find_next('div', class_='adFeatures')
                if extra_tags_div:
                    extra_tags = extra_tags_div.find_all('div', class_='adFeature')
                    for extra_tag in extra_tags:
                        tag_span = extra_tag.find('span')
                        if tag_span:
                            tag_value = tag_span.get_text(strip=True)
                            
                            # V√©rification des caract√©ristiques sp√©cifiques
                            if 'Ascenseur' in tag_value:
                                property_data['Ascenseur'] = 1
                            elif 'Garage' in tag_value or 'Parking' in tag_value:
                                property_data['Garage'] = 1
                            elif 'Jardin' in tag_value:
                                property_data['Jardin'] = 1
                            elif 'Piscine' in tag_value:
                                property_data['Piscine'] = 1
                            elif 'Cuisine √©quip√©e' in tag_value:
                                property_data['Cuisine_√©quip√©e'] = 1
                            elif 'Vue sur mer' in tag_value:
                                property_data['Vue_sur_mer'] = 1
                            elif 'Terrasse' in tag_value:
                                property_data['Terrasse'] = 1
                            elif 'S√©curit√©' in tag_value or 'Gardien' in tag_value:
                                property_data['S√©curit√©'] = 1
            
            return property_data
        else:
            print(f"√âchec de r√©cup√©ration de l'annonce: {link}. Code de statut: {response.status_code}")
            return property_data
    except Exception as e:
        print(f"Erreur lors de l'extraction des d√©tails de l'annonce {link}: {e}")
        return property_data

def main(pages=37):
    # URL de base pour les pages d'annonces immobili√®res √† Rabat
    base_url = 'https://www.mubawab.ma/fr/ct/rabat/immobilier-a-vendre'
    
    print("üè† D√©marrage du scraping des annonces immobili√®res √† Rabat...")
    # Initialisation d'une liste pour stocker tous les liens d'annonces
    all_listing_links = []
    # Nombre de pages √† scraper
    nb_pages = pages
    # R√©cup√©ration des liens d'annonces pour chaque page
    print(f"üìã Collecte des liens d'annonces sur {nb_pages} pages...")
    for page_number in tqdm(range(1, nb_pages + 1), desc="Progression"):
        page_url = f"{base_url}:p:{page_number}"
        links = get_listing_links(page_url)
        all_listing_links.extend(links)
        print(f"  Page {page_number}: {len(links)} liens collect√©s.")
 
    total_links = len(all_listing_links)
    print(f"‚úÖ Total des liens collect√©s: {total_links}")
    
    # Sauvegarde des liens dans un fichier CSV
    links_df = pd.DataFrame(all_listing_links, columns=['URL'])
    links_file = os.path.join(DATA_DIR, 'rabat_liens_annonces.csv')
    links_df.to_csv(links_file, index=False)
    
    # Extraction des d√©tails pour chaque annonce
    print("üîç Extraction des d√©tails des annonces...")
    all_property_data = []
    
    for idx, link in enumerate(tqdm(all_listing_links, desc="Progression")):
        print(f"  Traitement de l'annonce {idx+1}/{total_links}: {link}")
        property_data = extract_listing_details(link)
        all_property_data.append(property_data)
        
        # Sauvegarde interm√©diaire tous les 20 enregistrements
        if (idx + 1) % 20 == 0 or idx == len(all_listing_links) - 1:
            # Cr√©ation d'un DataFrame avec les donn√©es collect√©es
            data_df = pd.DataFrame(all_property_data)
            
            # Sauvegarde des donn√©es brutes dans un fichier CSV, sans aucun traitement
            output_file = os.path.join(DATA_DIR, 'rabat_donn√©es_immobili√®res_brutes.csv')
            data_df.to_csv(output_file, index=False)
            print(f"üíæ Sauvegarde interm√©diaire: {idx+1} annonces trait√©es et sauvegard√©es dans {output_file}")
    
    print("‚ú® Scraping termin√© avec succ√®s!")
    print(f"üìä Les donn√©es brutes ont √©t√© sauvegard√©es dans: {DATA_DIR}")
if __name__ == "__main__":
    import argparse
    
    # Configuration du parser d'arguments
    parser = argparse.ArgumentParser(description="Script de scraping des annonces immobili√®res √† Rabat")
    parser.add_argument("--pages", "-p", type=int, default=37, help="Nombre de pages √† scraper (d√©faut: 37)")
    
    # Analyse des arguments
    args = parser.parse_args()
    
    # Lancement du scraping avec le nombre de pages sp√©cifi√©
    main(pages=args.pages)